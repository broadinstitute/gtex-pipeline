#!/usr/bin/env python3
import numpy as np
import pandas as pd
import argparse
import os
from datetime import datetime
import pyBigWig
import multiprocessing as mp
import qtl.annotation
import qtl.norm
import qtl.io


def time_now():
    """Return time"""
    return datetime.now().strftime("%c")

#------------------------------------------------------------------------------
#  Preprocessing
#------------------------------------------------------------------------------
def prepare_annotation(annot_full, annot, exclude=['retained_intron', 'readthrough_transcript']):
    """
    Generate list of non-overlapping 3' UTRs for each gene

    Inputs
      annot_full: full GENCODE annotation
      annot: collapsed GENCODE annotation

    Outputs
      utr_df: dataframe with UTR coordinates. Positions are 1-based.
    """
    utr_df = []
    for k,g in enumerate(annot_full.genes, 1):
        # only consider genes in collapsed annotation
        if g.id in annot.gene_dict:
            # exons are always ordered 5' to 3' in direction of transcription
            utr_exons = [t.exons[-1] for t in g.transcripts if t.type not in exclude]
            collapsed_exons = annot.gene_dict[g.id].transcripts[0].exons
            # merge/collapse UTRs
            utr_coord = [[e.start_pos, e.end_pos] for e in utr_exons]
            utr_coord = qtl.annotation.interval_union(utr_coord)
            for utr in utr_coord:
                # find first matching exon in collapsed annotation (there can be multiple due to masked regions)
                cand = [ce for ce in collapsed_exons if np.minimum(ce.end_pos, utr[1]) > np.maximum(ce.start_pos, utr[0])]
                if cand:
                    # intersection with first match (in some cases, UTR is fragmented by collapsing procedure)
                    utr_df.append([g.chr,
                                   np.maximum(cand[0].start_pos, utr[0]),
                                   np.minimum(cand[0].end_pos, utr[1]),
                                   g.id,
                                   g.strand,
                                   annot.gene_dict[g.id].tss,
                                   cand[0].id
                                  ])

    utr_df = pd.DataFrame(utr_df, columns=['chr', 'start', 'end', 'gene_id', 'strand', 'tss', 'name'])
    utr_df['region_str'] = utr_df['chr']+':'+utr_df['start'].astype(str)+'-'+utr_df['end'].astype(str)
    return utr_df


def process_utrs(utr_annotation, chrom=None, min_length=150):
    """
    Prepare UTRs, filter out short UTRs

    utr_annotation: generated by prepare_annotation
    """
    utr_df = pd.read_csv(utr_annotation, sep='\t')

    utr_length = utr_df['end'] - utr_df['start'] + 1

    # shrink end by 20%
    end_shift = (0.2 * utr_length).round().astype(int)
    m = utr_df['strand'] == '+'
    utr_df.loc[m, 'end'] -= end_shift[m]
    utr_df.loc[~m, 'start'] += end_shift[~m]

    # drop short UTRs
    utr_df = utr_df[utr_length >= min_length]

    if chrom is not None:
        utr_df = utr_df[utr_df['chr'] == chrom]

    return utr_df.reset_index(drop=True)


#------------------------------------------------------------------------------
#  Core functions
#------------------------------------------------------------------------------
def load_coverage(chrom, start, end, strand, sample_df, verbose=False):
    """"""
    if 'bigwig_plus' in sample_df:  # stranded data
        if strand == '+':
            bigwig_col = 'bigwig_minus'  # opposite strand
        else:
            bigwig_col = 'bigwig_plus'
    else:  # unstranded
        bigwig_col = sample_df.columns[sample_df.columns.str.contains('bigwig')][0]

    coverage = np.zeros([sample_df.shape[0], end-start+1])
    # coverage = np.zeros([sample_df.shape[0], end-start])  # original
    for k,(i, bigwig) in enumerate(sample_df[bigwig_col].items()):
        if verbose:
            print(f'\r{k+1}/{sample_df.shape[0]}', end='')
        with pyBigWig.open(bigwig) as bw:
            # BED coordinates
            c = bw.values(chrom, start-1, end, numpy=True).astype(np.float32)
            # c = bw.values(chrom, start, end, numpy=True).astype(np.float32)  # original
            c[np.isnan(c)] = 0
        if strand == '-':
            c = c[::-1]
        coverage[k] = c
    return coverage


def compute_mse(coverage, break_point):
    """MSE for step function at break_point"""
    long_abund =  np.mean(coverage[:, break_point:], 1, keepdims=True)
    short_abund = np.mean(coverage[:, :break_point], 1, keepdims=True) - long_abund
    short_abund[short_abund < 0] = 0
    mse = np.mean(np.c_[coverage[:, :break_point] - long_abund - short_abund,
                        coverage[:, break_point:] - long_abund]**2)
    return mse, long_abund.squeeze(-1), short_abund.squeeze(-1)


def find_short_utr(coverages, UTR_start, UTR_end, strand,
                   coverage_weights, coverage_threshold=1,
                   min_coverage_samples_pct=0.3):
    """
    Fit a step function to UTR coverages to identify the most likely
    de novo alternative polyadenylation (APA) site.

      coverages: [samples x bases], ordered by direction of transcription
      utr_start: UTR start position (1-based)
      utr_end:   UTR end position
    """
    search_point_start = 150
    search_point_end = int(0.05 * abs(UTR_end - UTR_start))
    num_samples = coverages.shape[0]

    mask = np.mean(coverages[:, :99], axis=1) > coverage_threshold
    coverages_weighted = coverages[mask, :] / coverage_weights[mask]

    pass_threshold_index = np.where(mask)[0]

    if strand == "+":
        search_region = range(UTR_start + search_point_start, UTR_end - search_point_end + 1)
    else:
        search_region = range(UTR_end - search_point_start, UTR_start + search_point_end - 1, -1)
    n = len(search_region)

    # if sufficient samples have coverage, and UTR is at least 150bp long
    if coverages_weighted.shape[0] > num_samples * min_coverage_samples_pct and UTR_end - UTR_start >= 150 and n > 1:

        mse_list = np.zeros(n)
        long_abund = np.full([coverages_weighted.shape[0], n], np.nan)
        short_abund = np.full([coverages_weighted.shape[0], n], np.nan)
        # for each base in search region
        for k,curr_point in enumerate(range(search_point_start, UTR_end - UTR_start - search_point_end + 1)):
            mse_list[k], long_abund[:, k], short_abund[:, k] = compute_mse(coverages_weighted, curr_point)

        ix = mse_list.argmin()
        long_abund_full = np.full(num_samples, np.nan)
        long_abund_full[pass_threshold_index] = long_abund[:, ix]
        short_abund_full = np.full(num_samples, np.nan)
        short_abund_full[pass_threshold_index] = short_abund[:, ix]
        long_ratio = long_abund_full / (short_abund_full + long_abund_full)

        # mse, break point, long UTR ratio
        return mse_list[ix], search_region[ix], long_ratio, mse_list
    else:
        return None, None, None, None


def wrapper(args):
    """Wrapper for parallelization"""
    r, sample_df, coverage_weights, coverage_threshold = args
    coverage = load_coverage(r['chr'], r['start'], r['end'], r['strand'], sample_df)
    # compute optimal short UTR position
    mse, break_point, long_ratio, _ = find_short_utr(coverage, r['start'], r['end'],
                                                     r['strand'], coverage_weights,
                                                     coverage_threshold=coverage_threshold)

    if mse is not None:
        return [r['name'], mse, break_point, r['region_str']] + list(long_ratio)
    else:
        return None


def main(sample_df, utr_df, coverage_threshold=1, threads=4):
    """
    Inputs
      sample_df: dataframe
      utr_df: UTR annotation

    Outputs
    """
    if 'size_factor' in sample_df:
        coverage_weights = sample_df['size_factor'].values.reshape(-1,1)
    else:
        coverage_weights = (sample_df['lib_size'] / sample_df['lib_size'].mean()).values.reshape(-1,1)

    # iterate over UTRs
    dapars_df = []
    with mp.Pool(processes=threads) as pool:
        for k,res in enumerate(pool.imap_unordered(wrapper, [(r,sample_df,coverage_weights,coverage_threshold) for _,r in utr_df.iterrows()]), 1):
            if res is not None:
                dapars_df.append(res)
            print(f"\rProcessing 3' UTR {k}/{utr_df.shape[0]}", end='')
        print()

    header = ['name', 'fit_value', 'predicted_proximal_apa', 'locus'] + sample_df.index.tolist()
    dapars_df = pd.DataFrame(dapars_df, columns=header)
    return dapars_df


def make_bed(dapars_df, utr_df, participant_s=None, nan_frac_threshold=0.5):
    """
    Convert to BED format, with start/end corresponding to TSS
    """
    gene_df = utr_df[['gene_id', 'chr', 'tss']].drop_duplicates().set_index('gene_id')

    # convert to BED format
    bed_df = dapars_df.copy()
    bed_df.insert(0, 'gene_id', bed_df['name'].map(lambda x: x.split('_')[0]))
    bed_df.insert(0, 'chr', bed_df['gene_id'].map(gene_df['chr']))
    bed_df.insert(1, 'start', bed_df['gene_id'].map(gene_df['tss'])-1)
    bed_df.insert(2, 'end', bed_df['gene_id'].map(gene_df['tss']))
    bed_df.insert(3, 'phenotype_id', bed_df['gene_id'] + '_' + bed_df['locus'])

    group_s = bed_df.set_index('phenotype_id')['gene_id']
    bed_df.drop(['gene_id', 'name', 'fit_value', 'predicted_proximal_apa', 'locus'], axis=1, inplace=True)

    # sort
    bed_df.sort_values(['chr', 'start', 'end'], key=lambda x:
                       x.str.replace('chr','').str.replace('X','23').astype(int) if x.dtype == object else x,
                       inplace=True)
    bed_df.reset_index(drop=True, inplace=True)

    print(f'Mapped phenotypes: {bed_df.shape[0]}')
    # drop phenotypes with > 50% NA
    cols = bed_df.columns[4:]
    m = bed_df[cols].isnull().sum(1) < nan_frac_threshold*len(cols)
    bed_df = bed_df[m]
    print(f'Phenotypes with < {nan_frac_threshold*100:.0f}% NA: {bed_df.shape[0]}')

    # low complexity filter
    n = np.floor(len(cols)*0.1)
    if n < 10:
        n = 10
    n_unique = bed_df[cols].apply(lambda x: x.nunique(dropna=True), axis=1)
    bed_df = bed_df[n_unique >= n].reset_index(drop=True)
    print(f'Phenotypes with â‰¥ {n} unique values: {bed_df.shape[0]}')

    # mean imputation
    tmp_df = bed_df[cols].copy()
    for x in tmp_df.values:
        m = np.isnan(x)
        x[m] = x[~m].mean()

    # standardize
    tmp_df = tmp_df - np.mean(tmp_df.values, axis=1, keepdims=True)
    tmp_df = tmp_df / np.std(tmp_df.values, axis=1, keepdims=True)
    # inverse normal transform on columns (as in Degner et al., 2012 and Li et al., 2017)
    tmp_df = qtl.norm.inverse_normal_transform(tmp_df.T).T

    bed_df[cols] = tmp_df[cols]

    # rename columns (participant IDs)
    if participant_s is not None:
        bed_df.rename(columns=participant_s, inplace=True)

    return bed_df, group_s.loc[bed_df['phenotype_id']]


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Reimplementation of DaPars2, enabling testing of multiple 3' UTRs per gene")
    parser.add_argument('--bigwig_files', required=True,
                        help='TSV file with columns sample_id, bigwig (or bigwig_plus and bigwig_minus for stranded data)')
    parser.add_argument('--utr', required=True, help='UTR annotation (1-based coordinates)')
    parser.add_argument('--size_factors', required=True, help='DESeq size factors for between sample normalization')
    parser.add_argument('--prefix', required=True, help='Output file prefix')
    parser.add_argument('--sample_participant_lookup', default=None, help='Lookup table (TSV) linking samples to participants')
    parser.add_argument('--expression_bed', default=None)
    parser.add_argument('--coverage_threshold', type=int, default=1, help='')
    parser.add_argument('--chr', type=str, default=None, help='')
    parser.add_argument('--threads', type=int, default=1, help='Number of threads')
    parser.add_argument('--output_dir', default='.')
    parser.add_argument('--parquet', action='store_true', help='Save output in parquet format')
    args = parser.parse_args()

    print(f"[{time_now()}] Running DaPars2")
    print(f"  * coverage threshold: {args.coverage_threshold}")

    sample_df = pd.read_csv(args.bigwig_files, sep='\t', index_col=0)
    sizefactor_s = pd.read_csv(args.size_factors, sep='\t', index_col=0).squeeze('columns')
    assert sample_df.index.isin(sizefactor_s.index).all()
    sample_df = sample_df.join(sizefactor_s)
    print(f"  * {len(sample_df)} samples")

    utr_df = process_utrs(args.utr, chrom=args.chr)  # run this here in case a chromosome is selected
    print(f"  * {len(utr_df)} annotated 3' UTRs")

    # subset genes
    if args.expression_bed is not None:
        if args.expression_bed.endswith('parquet'):
            expression_df = pd.read_parquet(args.expression_bed)
        else:
            expression_df = pd.read_csv(args.expression_bed, sep='\t')
        utr_df = utr_df[utr_df['gene_id'].isin(expression_df['gene_id'])]
        print(f"  * {len(utr_df)} 3' UTRs after expression filter")

    dapars_df = main(sample_df, utr_df, coverage_threshold=args.coverage_threshold, threads=args.threads)
    if args.sample_participant_lookup is not None:
        sample_participant_lookup_s = pd.read_csv(args.sample_participant_lookup,
                                                  sep='\t', index_col=0, header=None, dtype=str).squeeze('columns')
    else:
        sample_participant_lookup_s = None
    bed_df, group_s = make_bed(dapars_df, utr_df, participant_s=sample_participant_lookup_s, nan_frac_threshold=0.5)

    if args.parquet:
        dapars_df.to_parquet(os.path.join(args.output_dir, f"{args.prefix}.dapars2_3p_UTR_ratios.parquet"), index=False)
        bed_df.to_parquet(os.path.join(args.output_dir, f"{args.prefix}.dapars2_phenotypes.bed.parquet"))
    else:
        dapars_df.to_csv(os.path.join(args.output_dir, f"{args.prefix}.dapars2_3p_UTR_ratios.txt.gz"),
                         sep='\t', float_format='%.2f', index=False, na_rep='NA')
        qtl.io.write_bed(bed_df, os.path.join(args.output_dir, f"{args.prefix}.dapars2_phenotypes.bed.gz"))
    group_s.to_csv(os.path.join(args.output_dir, f"{args.prefix}.dapars2_phenotype_groups.txt.gz"), sep='\t', header=False)

    print(f"[{time_now()}] Finished")
